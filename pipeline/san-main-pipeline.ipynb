{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4986,"databundleVersionId":860641,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Overview** <br>\nWorking with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience.<br>\n\nThe \"TARGET\" column is the variable to predict. It equals one for unsatisfied customers and 0 for satisfied customers.\n\nThe goal is to predict the probability that each customer in the test set is an unsatisfied customer.\n\n**Data File descriptions**<br>\ntrain.csv - the training set including the target<br>\ntest.csv - the test set without the target","metadata":{}},{"cell_type":"markdown","source":"****Import Libraries****","metadata":{}},{"cell_type":"code","source":"# Data processing\nimport numpy as np\nimport pandas as pd\nimport random\nimport itertools\nfrom scipy import stats\nfrom scipy.sparse import hstack\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.font_manager as fm\n\n# Feature Engineering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_curve, roc_auc_score, log_loss\n\n# System / Settings\nfrom tqdm import tqdm\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nsns.set(palette='muted', style='whitegrid')\nnp.random.seed(13154)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:07.467739Z","iopub.execute_input":"2025-12-10T08:46:07.468123Z","iopub.status.idle":"2025-12-10T08:46:07.479600Z","shell.execute_reply.started":"2025-12-10T08:46:07.468101Z","shell.execute_reply":"2025-12-10T08:46:07.478190Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****Load Data****","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')\ntest = pd.read_csv('/kaggle/input/santander-customer-satisfaction/test.csv')\nprint(\"Number of training samples: %i, number of features: %i\" % (train.shape[0], train.shape[1]))\nprint(\"Number of test samples: %i, number of features: %i\" % (test.shape[0], test.shape[1]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:07.498013Z","iopub.execute_input":"2025-12-10T08:46:07.498336Z","iopub.status.idle":"2025-12-10T08:46:11.692751Z","shell.execute_reply.started":"2025-12-10T08:46:07.498313Z","shell.execute_reply":"2025-12-10T08:46:11.690966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:11.695089Z","iopub.execute_input":"2025-12-10T08:46:11.695506Z","iopub.status.idle":"2025-12-10T08:46:11.720058Z","shell.execute_reply.started":"2025-12-10T08:46:11.695480Z","shell.execute_reply":"2025-12-10T08:46:11.718608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:11.721401Z","iopub.execute_input":"2025-12-10T08:46:11.721789Z","iopub.status.idle":"2025-12-10T08:46:11.759002Z","shell.execute_reply.started":"2025-12-10T08:46:11.721758Z","shell.execute_reply":"2025-12-10T08:46:11.757878Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**EDA & Feature Engineering**","metadata":{}},{"cell_type":"code","source":"# Filter zero-variance features\ni = 0\nfor col in train.columns:\n    if train[col].var() == 0:\n        i += 1\n        del train[col]\n        del test[col]\nprint(\"%i features with zero variance have been removed\" % (i))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:11.761143Z","iopub.execute_input":"2025-12-10T08:46:11.761706Z","iopub.status.idle":"2025-12-10T08:46:12.032600Z","shell.execute_reply.started":"2025-12-10T08:46:11.761663Z","shell.execute_reply":"2025-12-10T08:46:12.031561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter sparse features\ni = 0\nfor col in train.columns:\n    if np.percentile(train[col], 99) == 0:\n        i += 1\n        del train[col]\n        del test[col]\n        \nprint(\"%i sparse features have been removed\" % (i))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:12.033750Z","iopub.execute_input":"2025-12-10T08:46:12.034108Z","iopub.status.idle":"2025-12-10T08:46:13.109257Z","shell.execute_reply.started":"2025-12-10T08:46:12.034087Z","shell.execute_reply":"2025-12-10T08:46:13.108278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Get pairwise combinations of all columns\ncombinations = list(itertools.combinations(train.columns, 2))\nprint(combinations[:20])\nlen(combinations)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:13.110512Z","iopub.execute_input":"2025-12-10T08:46:13.110830Z","iopub.status.idle":"2025-12-10T08:46:13.119475Z","shell.execute_reply.started":"2025-12-10T08:46:13.110808Z","shell.execute_reply":"2025-12-10T08:46:13.118328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove duplicate features, keeping one copy\nimport itertools\n\ncombinations = list(itertools.combinations(train.columns, 2))\nremove = []\nkeep = []\n\nfor f1, f2 in combinations:\n    if (f1 not in remove) & (f2 not in remove):\n        if train[f1].equals(train[f2]):\n            remove.append(f1)\n            keep.append(f2)\n\ntrain.drop(remove, axis=1, inplace=True)\ntest.drop(remove, axis=1, inplace=True)\nprint(\"%i features are duplicates, and %i features have been removed\" % (len(remove)*2, len(remove)))\nprint(\"Features %s were removed\\nFeatures %s were retained\" % (remove, keep))\n\ndel remove\ndel keep\ndel combinations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:13.120937Z","iopub.execute_input":"2025-12-10T08:46:13.121273Z","iopub.status.idle":"2025-12-10T08:46:14.295005Z","shell.execute_reply.started":"2025-12-10T08:46:13.121241Z","shell.execute_reply":"2025-12-10T08:46:14.294008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.shape, test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:14.295898Z","iopub.execute_input":"2025-12-10T08:46:14.296180Z","iopub.status.idle":"2025-12-10T08:46:14.303136Z","shell.execute_reply.started":"2025-12-10T08:46:14.296154Z","shell.execute_reply":"2025-12-10T08:46:14.302371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.isnull().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:14.304365Z","iopub.execute_input":"2025-12-10T08:46:14.304712Z","iopub.status.idle":"2025-12-10T08:46:14.346776Z","shell.execute_reply.started":"2025-12-10T08:46:14.304682Z","shell.execute_reply":"2025-12-10T08:46:14.345309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.isnull().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:14.350875Z","iopub.execute_input":"2025-12-10T08:46:14.351157Z","iopub.status.idle":"2025-12-10T08:46:14.384202Z","shell.execute_reply.started":"2025-12-10T08:46:14.351137Z","shell.execute_reply":"2025-12-10T08:46:14.382884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Processing VAR3 features\ntrain['var3'].replace(-999999, 2, inplace=True)\ntest['var3'].replace(-999999, 2, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:14.385217Z","iopub.execute_input":"2025-12-10T08:46:14.385551Z","iopub.status.idle":"2025-12-10T08:46:14.394623Z","shell.execute_reply.started":"2025-12-10T08:46:14.385527Z","shell.execute_reply":"2025-12-10T08:46:14.393336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Processing VAR15 features\nfor df in [train, test]:\n    df['var15_below_23'] = np.zeros(df.shape[0], dtype=int)\n    df.loc[df['var15'] < 23, 'var15_below_23'] = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:14.395649Z","iopub.execute_input":"2025-12-10T08:46:14.396002Z","iopub.status.idle":"2025-12-10T08:46:14.419907Z","shell.execute_reply.started":"2025-12-10T08:46:14.395969Z","shell.execute_reply":"2025-12-10T08:46:14.418780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Processing VAR38 features\nvar38_q975 = np.quantile(train['var38'], 0.975)\nfor df in [train, test]:\n    df['var38_clipped'] = df['var38'].clip(upper=var38_q975)\n    df['var38_log'] = np.log1p(df['var38_clipped'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:14.421177Z","iopub.execute_input":"2025-12-10T08:46:14.421638Z","iopub.status.idle":"2025-12-10T08:46:14.449417Z","shell.execute_reply.started":"2025-12-10T08:46:14.421602Z","shell.execute_reply":"2025-12-10T08:46:14.448268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Processing other features with ”imp“\nimp = [col for col in train.columns if 'imp' in col]\n\nfor df in [train, test]:\n    for col in imp:\n        if col in df.columns:\n            mask = df[col] > 0  \n            df.loc[mask, col] = np.log(df.loc[mask, col])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:14.450310Z","iopub.execute_input":"2025-12-10T08:46:14.450586Z","iopub.status.idle":"2025-12-10T08:46:14.530985Z","shell.execute_reply.started":"2025-12-10T08:46:14.450566Z","shell.execute_reply":"2025-12-10T08:46:14.530055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Processing other features with ”saldo“\nsaldo = [col for col in train.columns if 'saldo' in col]\n\nfor df in [train, test]:\n    for col in saldo:\n        if col in df.columns:\n            mask = df[col] > 0  \n            df.loc[mask, col] = np.log(df.loc[mask, col])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:14.531952Z","iopub.execute_input":"2025-12-10T08:46:14.532222Z","iopub.status.idle":"2025-12-10T08:46:14.674231Z","shell.execute_reply.started":"2025-12-10T08:46:14.532202Z","shell.execute_reply":"2025-12-10T08:46:14.672957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Processing other features with ”num“\n\nTHRESHOLD = 10\n\nnum = [\n    col for col in train.columns \n    if col.startswith('num') and \n    max(train[col].nunique(), test[col].nunique()) <= THRESHOLD\n]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:14.675385Z","iopub.execute_input":"2025-12-10T08:46:14.675887Z","iopub.status.idle":"2025-12-10T08:46:14.760733Z","shell.execute_reply.started":"2025-12-10T08:46:14.675830Z","shell.execute_reply":"2025-12-10T08:46:14.759595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#save as p-file\ntrain.to_pickle('/kaggle/working/train.pkl')\ntest.to_pickle('/kaggle/working/test.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:14.761665Z","iopub.execute_input":"2025-12-10T08:46:14.761948Z","iopub.status.idle":"2025-12-10T08:46:15.047005Z","shell.execute_reply.started":"2025-12-10T08:46:14.761928Z","shell.execute_reply":"2025-12-10T08:46:15.045882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"特征工程","metadata":{}},{"cell_type":"code","source":"\ntrain = pd.read_pickle('/kaggle/working/train.pkl')\ntest = pd.read_pickle('/kaggle/working/test.pkl')\nX_train = train.copy()\nX_test = test.copy()\nX_train.shape, X_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:15.047994Z","iopub.execute_input":"2025-12-10T08:46:15.048377Z","iopub.status.idle":"2025-12-10T08:46:15.266933Z","shell.execute_reply.started":"2025-12-10T08:46:15.048347Z","shell.execute_reply":"2025-12-10T08:46:15.265949Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Create new features**","metadata":{}},{"cell_type":"code","source":"def add_feature_no_zeros(train=X_train, test=X_test):\n    \"\"\"Construct a new feature to represent the number of times each of the 143 features takes a value of zero or non-zero in each row of samples.\"\"\"\n    \n    # Get all columns except 'TARGET'\n    col = [k for k in train.columns if k != 'TARGET']\n    \n    for df in [train, test]:\n        df['no_zeros'] = (df.loc[:, col] == 0).sum(axis=1).values\n        df['no_nonzeros'] = (df.loc[:, col] != 0).sum(axis=1).values\n    \n    print(\"Added zero/non-zero count features\")\n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:56.802760Z","iopub.execute_input":"2025-12-10T08:46:56.803389Z","iopub.status.idle":"2025-12-10T08:46:56.809934Z","shell.execute_reply.started":"2025-12-10T08:46:56.803346Z","shell.execute_reply":"2025-12-10T08:46:56.808599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_feature_no_zeros_keyword(keyword, train=X_train, test=X_test):\n    \"\"\"Construct a new feature representing the number of times each keyword in each row of samples can have a feature value of zero or non-zero.\"\"\"\n    col = [k for k in train.columns if keyword in k]\n    for df in [train, test]:\n        df['no_zeros_' + keyword] = (df.loc[:, col] == 0).sum(axis=1).values\n        df['no_nonzeros_' + keyword] = (df.loc[:, col] != 0).sum(axis=1).values\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:46:59.845001Z","iopub.execute_input":"2025-12-10T08:46:59.845357Z","iopub.status.idle":"2025-12-10T08:46:59.852155Z","shell.execute_reply.started":"2025-12-10T08:46:59.845329Z","shell.execute_reply":"2025-12-10T08:46:59.850924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"add_feature_no_zeros()\nkeywords = ['imp', 'saldo', 'num', 'ind']\nfor k in keywords:\n    add_feature_no_zeros_keyword(k)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:47:02.773529Z","iopub.execute_input":"2025-12-10T08:47:02.773896Z","iopub.status.idle":"2025-12-10T08:47:03.399310Z","shell.execute_reply.started":"2025-12-10T08:47:02.773853Z","shell.execute_reply":"2025-12-10T08:47:03.398417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def average_col(col, features, train=X_train, test=X_test):\n    \"\"\"\nObtain the mean of the feature for each unique value in the 'col' feature, and set it as the new feature.\n    \"\"\"\n\n    for df in [train, test]:\n        unique_values = df[col].unique()\n\n        for feature in features:\n            avg_value = []\n            for value in unique_values:\n                # For each feature column col, calculate the mean of the feature for each unique value.\n                avg = df.loc[df[col] == value, feature].mean()\n                avg_value.append(avg)\n            avg_dict = dict(zip(unique_values, avg_value))\n            new_col = 'avg_' + col + '_' + feature\n            df[new_col] = np.zeros(df.shape[0])\n            for value in unique_values:\n                df.loc[df[col] == value, new_col] = avg_dict[value]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:47:07.730340Z","iopub.execute_input":"2025-12-10T08:47:07.730854Z","iopub.status.idle":"2025-12-10T08:47:07.738234Z","shell.execute_reply.started":"2025-12-10T08:47:07.730825Z","shell.execute_reply":"2025-12-10T08:47:07.737347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# All features containing the prefixes imp and saldo exclude no_zeros_imp and no_zeros_saldo\nfeatures = [i for i in X_train.columns if (('imp' in i) or ('saldo' in i)) & ('no_zeros' not in i)]\n\n# Find columns where the number of unique values ​​is between 50 and 210\ncolumns = [i for i in X_train.columns if (X_train[i].nunique() <= 210) & (X_train[i].nunique() > 50)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:47:10.086586Z","iopub.execute_input":"2025-12-10T08:47:10.086937Z","iopub.status.idle":"2025-12-10T08:47:10.320733Z","shell.execute_reply.started":"2025-12-10T08:47:10.086908Z","shell.execute_reply":"2025-12-10T08:47:10.319774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nfor col in tqdm(columns):\n    average_col(col,features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:47:12.037686Z","iopub.execute_input":"2025-12-10T08:47:12.038041Z","iopub.status.idle":"2025-12-10T08:51:48.265442Z","shell.execute_reply.started":"2025-12-10T08:47:12.038015Z","shell.execute_reply":"2025-12-10T08:51:48.264182Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Choose features**","metadata":{}},{"cell_type":"code","source":"def remove_corr_var(train=X_train, test=X_test, target_threshold=10**-3, within_threshold=0.95):\n    \"\"\"\n    Remove features with low correlation to the target variable, \n    remove highly correlated features among themselves (keep one)\n    \"\"\"\n    # Remove features with low correlation to the target variable\n    initial_feature = train.shape[1]\n    corr = train.drop(\"ID\", axis=1).corr().abs().T\n    corr_target = pd.DataFrame(corr['TARGET'].sort_values())  # Remove the by='TARGET' parameter\n    feat_df = corr_target[(corr_target['TARGET']) <= target_threshold]\n    print(\"%i features removed because the absolute correlation coefficient with target variable TARGET is less than %.3f\" % (feat_df.shape[0], target_threshold))\n    print(\"Removing in progress......\")\n    for df in [train, test]:\n        df.drop(feat_df.index, axis=1, inplace=True)\n    print(\"Removal completed!\")\n    \n    # Remove highly correlated features among themselves (keep the one with highest correlation to TARGET)\n    corr.sort_values(by='TARGET', ascending=False, inplace=True)  # Sort each row of correlation matrix in descending order by TARGET column\n    corr = corr.reindex(columns=corr.index)  # Reorder each column according to row index\n    corr.drop('TARGET', axis=1, inplace=True)  # Remove TARGET column\n    corr.drop('TARGET', axis=0, inplace=True)\n    corr.drop(feat_df.index, axis=1, inplace=True)  # Remove features from feat_df in corr table columns\n    corr.drop(feat_df.index, inplace=True)\n    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))  # Get upper triangle of correlation matrix\n    column = [col for col in upper.columns if any(upper[col] > within_threshold)]  # Get all columns highly correlated with one of the features\n    print(\"%i features removed because they are highly correlated with another feature with correlation coefficient %.3f or higher\" % (len(column), within_threshold))\n    print(\"Removing in progress......\")\n    for df in [train, test]:\n        df.drop(column, axis=1, inplace=True)\n    print(\"Removal completed!\")\n    print(\"Number of features changed from %i to %i, where %i features have been removed\" %\n          (initial_feature, test.shape[1], initial_feature - test.shape[1]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:51:55.660802Z","iopub.execute_input":"2025-12-10T08:51:55.661159Z","iopub.status.idle":"2025-12-10T08:51:55.673206Z","shell.execute_reply.started":"2025-12-10T08:51:55.661133Z","shell.execute_reply":"2025-12-10T08:51:55.671962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nremove_corr_var()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T08:52:00.179318Z","iopub.execute_input":"2025-12-10T08:52:00.179778Z","iopub.status.idle":"2025-12-10T08:55:28.309103Z","shell.execute_reply.started":"2025-12-10T08:52:00.179754Z","shell.execute_reply":"2025-12-10T08:55:28.308009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nX_train.to_pickle('/kaggle/working/X_train.pkl')\nX_test.to_pickle('/kaggle/working/X_test.pkl')\n\nX_train = pd.read_pickle('/kaggle/working/X_train.pkl')\nX_test = pd.read_pickle('/kaggle/working/X_test.pkl')\nX_train.shape, X_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:01:39.370246Z","iopub.execute_input":"2025-12-10T09:01:39.370591Z","iopub.status.idle":"2025-12-10T09:01:39.811820Z","shell.execute_reply.started":"2025-12-10T09:01:39.370571Z","shell.execute_reply":"2025-12-10T09:01:39.810925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Logarithmic transformation of imp and saldo**","metadata":{}},{"cell_type":"code","source":"def apply_loglp(column, train=X_train, test=X_test):\n    \"\"\"\n    Logarithmic transformation of all column features\n    \"\"\"\n    tr = train.copy()\n    te = test.copy()\n    for df in [tr, te]:\n        for col in column:\n            df.loc[df[col] >= 0, col] = np.log1p(df.loc[df[col] >= 0, col].values)\n    return tr, te\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:01:43.282409Z","iopub.execute_input":"2025-12-10T09:01:43.282738Z","iopub.status.idle":"2025-12-10T09:01:43.289119Z","shell.execute_reply.started":"2025-12-10T09:01:43.282716Z","shell.execute_reply":"2025-12-10T09:01:43.287841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform a logarithmic transformation on all imp and saldo features with a minimum value greater than or equal to 0 (var38 has already been logarithmically transformed in EDA, so it is not performed here).\nfeatures = [i for i in X_train.columns if (('saldo' in i) | ('imp' in i)) & ((X_train[i].values >= 0).all())]\nX_train_1, X_test_1 = apply_loglp(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:01:46.247137Z","iopub.execute_input":"2025-12-10T09:01:46.247522Z","iopub.status.idle":"2025-12-10T09:01:47.008405Z","shell.execute_reply.started":"2025-12-10T09:01:46.247426Z","shell.execute_reply":"2025-12-10T09:01:47.007329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nX_train_1.to_pickle('/kaggle/working/X_train_1.pkl')\nX_test_1.to_pickle('/kaggle/working/X_test_1.pkl')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:02:14.448549Z","iopub.execute_input":"2025-12-10T09:02:14.448938Z","iopub.status.idle":"2025-12-10T09:02:14.711893Z","shell.execute_reply.started":"2025-12-10T09:02:14.448916Z","shell.execute_reply":"2025-12-10T09:02:14.710810Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**One-hot**","metadata":{}},{"cell_type":"code","source":"# Select features whose unique values ​​are in the range (2, 10].\ncat_col = []\nfor col in X_train.columns:\n    if (X_train[col].nunique() <= 10) & (col != 'TARGET') & (X_train[col].nunique() > 2):\n        cat_col.append(col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:02:17.163818Z","iopub.execute_input":"2025-12-10T09:02:17.164194Z","iopub.status.idle":"2025-12-10T09:02:17.729854Z","shell.execute_reply.started":"2025-12-10T09:02:17.164170Z","shell.execute_reply":"2025-12-10T09:02:17.728897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def one_hot_encoding(col, train=X_train, test=X_test):\n    \"\"\"One-hot encoding of features in the training and test sets.\n    \"\"\" \n    ohe = OneHotEncoder(sparse=True, handle_unknown='ignore')\n    ohe.fit(train[col])\n    feature_names = list(ohe.get_feature_names_out(input_features=col))\n    features = list(train.drop(col, axis=1).columns)\n    features.extend(feature_names)\n\n    # train\n    df = train.copy()\n    temp = ohe.transform(df[col])\n    df.drop(col, axis=1, inplace=True)\n    train = pd.DataFrame(hstack([df.values, temp]).toarray(), columns=features)\n    train = train.loc[:, ~train.columns.duplicated(keep='first')] # 删除重复列\n    \n    # test\n    df = test.copy()\n    temp = ohe.transform(df[col])\n    df.drop(col, axis=1, inplace=True)\n    features.remove('TARGET')\n    test = pd.DataFrame(hstack([df.values, temp]).toarray(), columns=features)\n    test = test.loc[:, ~test.columns.duplicated(keep='first')]\n\n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:02:19.454614Z","iopub.execute_input":"2025-12-10T09:02:19.455447Z","iopub.status.idle":"2025-12-10T09:02:19.465808Z","shell.execute_reply.started":"2025-12-10T09:02:19.455405Z","shell.execute_reply":"2025-12-10T09:02:19.464708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_ohe, X_test_ohe = one_hot_encoding(cat_col)\nX_train_1_ohe, X_test_1_ohe = one_hot_encoding(cat_col, X_train_1, X_test_1)\nX_train_ohe.shape, X_test_ohe.shape, X_train_1_ohe.shape, X_test_1_ohe.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:02:22.830352Z","iopub.execute_input":"2025-12-10T09:02:22.830717Z","iopub.status.idle":"2025-12-10T09:02:30.729257Z","shell.execute_reply.started":"2025-12-10T09:02:22.830692Z","shell.execute_reply":"2025-12-10T09:02:30.728229Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Response encoding**","metadata":{}},{"cell_type":"code","source":"def response_encoding_return(df, column, target, alpha=5000):\n    \"\"\"\n    This function encodes the response with Laplacian smoothing into a categorical column and transforms the corresponding column in the training, testing, and validation datasets.\n    This function is used to train the optimal parameter alpha.\n    \"\"\"\n    unique_values = set(df[column].values)\n    dict_values = {}\n    for value in unique_values:\n        total = len(df[df[column] == value])\n        sum_promoted = len(df[(df[column] == value) & (df[target] == 1)]) \n        dict_values[value] = np.round((sum_promoted + alpha) / (total + alpha * len(unique_values)), 2)\n    return dict_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:02:35.832028Z","iopub.execute_input":"2025-12-10T09:02:35.832358Z","iopub.status.idle":"2025-12-10T09:02:35.838624Z","shell.execute_reply.started":"2025-12-10T09:02:35.832333Z","shell.execute_reply":"2025-12-10T09:02:35.837296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def response_encoding(df, test_df, column, target='TARGET', alpha=5000):\n    \"\"\"\n    Here we use response encoding with Laplace smoothing on categorical columns, \n    and transform the corresponding columns in training, testing, and validation datasets.\n    In this method, we repeat each category value alpha times.\n    \"\"\" \n    feature_1 = column + '_1'\n    feature_0 = column + '_0'\n    \n    unique_values = set(df[column].values)\n    dict_values_1 = {}  # Store response encoding values for target=1\n    dict_values_0 = {}  # Store response encoding values for target=0\n\n    for value in unique_values:\n        total = len(df[df[column] == value])  # Total count of this category value in df\n        # Count when category is 'value' and target variable is 1 in df\n        sum_promoted = len(df[(df[column] == value) & (df[target] == 1)])\n        sum_unpromoted = total - sum_promoted  # Count when category is 'value' and target variable is 0 in df\n        \n        # Laplace smoothing\n        dict_values_1[value] = np.round((sum_promoted + alpha) / (total + alpha * len(unique_values)), 2)\n        dict_values_0[value] = np.round((sum_unpromoted + alpha) / (total + alpha * len(unique_values)), 2)\n    \n    dict_values_1['unknown'] = 0.5  # Unknown categories not observed in training set will be assigned 0.5\n    dict_values_0['unknown'] = 0.5\n    \n    df[feature_1] = (df[column].map(dict_values_1)).values\n    df[feature_0] = (df[column].map(dict_values_0)).values\n    df.drop(column, axis=1, inplace=True)\n\n    unique_values_test = set(test_df[column].values)\n    # Find different elements between two sets and assign them as 'unknown'\n    test_df[column] = test_df[column].apply(lambda x: 'unknown' if x in (unique_values_test - unique_values) else x)\n    test_df[feature_1] = (test_df[column].map(dict_values_1)).values\n    test_df[feature_0] = (test_df[column].map(dict_values_0)).values\n    test_df.drop(column, axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:02:38.196148Z","iopub.execute_input":"2025-12-10T09:02:38.196469Z","iopub.status.idle":"2025-12-10T09:02:38.206522Z","shell.execute_reply.started":"2025-12-10T09:02:38.196444Z","shell.execute_reply":"2025-12-10T09:02:38.205254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"alpha = 100\nX_train_re = X_train.copy()\nX_test_re = X_test.copy()\nX_train_1_re = X_train_1.copy()\nX_test_1_re = X_test_1.copy()\nfor col in tqdm(cat_col):\n    response_encoding(X_train_re, X_test_re, col, alpha=alpha)\n    response_encoding(X_train_1_re, X_test_1_re, col, alpha=alpha)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:02:41.339940Z","iopub.execute_input":"2025-12-10T09:02:41.340259Z","iopub.status.idle":"2025-12-10T09:02:51.819918Z","shell.execute_reply.started":"2025-12-10T09:02:41.340238Z","shell.execute_reply":"2025-12-10T09:02:51.818658Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Standardize features**","metadata":{}},{"cell_type":"code","source":"def stdzation(train, test):\n    \"\"\"\n    Standardize features\n    \"\"\"\n    col = [i for i in train.columns if (i != 'TARGET') & (i != 'ID')]\n    scaler = StandardScaler()\n    train[col] = scaler.fit_transform(train[col])\n    test[col] = scaler.transform(test[col])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:02:56.373999Z","iopub.execute_input":"2025-12-10T09:02:56.374346Z","iopub.status.idle":"2025-12-10T09:02:56.380832Z","shell.execute_reply.started":"2025-12-10T09:02:56.374322Z","shell.execute_reply":"2025-12-10T09:02:56.379304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasets = [\n    (X_train, X_test), \n    (X_train_re, X_test_re), \n    (X_train_ohe, X_test_ohe),\n    (X_train_1, X_test_1), \n    (X_train_1_re, X_test_1_re), \n    (X_train_1_ohe, X_test_1_ohe)\n]\n\nfor train, test in datasets:\n    stdzation(train, test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:02:58.532280Z","iopub.execute_input":"2025-12-10T09:02:58.532576Z","iopub.status.idle":"2025-12-10T09:03:06.384789Z","shell.execute_reply.started":"2025-12-10T09:02:58.532556Z","shell.execute_reply":"2025-12-10T09:03:06.383943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasets_labels = ['normal', 'normal_re', \"normal_ohe\", \"log\", 'log_re', \"log_ohe\"]\nprint(\"The final number of features for different datasets is\")\nfor i, (train, test) in enumerate(datasets):\n    print(\"%s:\\t%i\" % (datasets_labels[i], test.shape[1]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:03:10.610720Z","iopub.execute_input":"2025-12-10T09:03:10.611154Z","iopub.status.idle":"2025-12-10T09:03:10.617942Z","shell.execute_reply.started":"2025-12-10T09:03:10.611124Z","shell.execute_reply":"2025-12-10T09:03:10.616648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, (train, test) in enumerate(datasets):\n    file = datasets_labels[i] + '.pkl'\n    \n    train.to_pickle('/kaggle/working/train_' + file)\n    test.to_pickle('/kaggle/working/test_' + file)\n    \n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:03:52.277554Z","iopub.execute_input":"2025-12-10T09:03:52.277936Z","iopub.status.idle":"2025-12-10T09:03:54.430843Z","shell.execute_reply.started":"2025-12-10T09:03:52.277913Z","shell.execute_reply":"2025-12-10T09:03:54.429803Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Modeling**","metadata":{}},{"cell_type":"code","source":"# Load the dataset from /kaggle/working/\ndataset = 'Normal'\ntrain = pd.read_pickle('/kaggle/working/train_normal.pkl')  \ntest = pd.read_pickle('/kaggle/working/test_normal.pkl')    \n\nX_train = train.drop(['ID', 'TARGET'], axis=1)\ny_train = train['TARGET'].values\nX_test = test.drop('ID', axis=1)\ntest_id = test['ID']\ndel train, test\n\n# Split the dataset\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.15)\nprint(f\"Shapes: X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:04:32.076606Z","iopub.execute_input":"2025-12-10T09:04:32.077057Z","iopub.status.idle":"2025-12-10T09:04:33.173321Z","shell.execute_reply.started":"2025-12-10T09:04:32.077018Z","shell.execute_reply":"2025-12-10T09:04:33.172310Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"global i\ni = 0\n\ndef plot_auc(y_true, y_pred, label, dataset=dataset):\n    \"\"\" \n    Plot ROC curve given y_true and y_pred\n    dataset: tells us which dataset is being used\n    label: tells us which model is being used; if label is a list, plot ROC curves for all labels\n    \"\"\"\n    from sklearn.metrics import roc_auc_score, roc_curve, log_loss\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import numpy as np\n    import pandas as pd\n    \n    if not isinstance(label, (list, np.ndarray)):\n        print(f\"\\t'{label} on {dataset} dataset'\\t\\t\\n\")\n        auc = roc_auc_score(y_true, y_pred)\n        logloss = log_loss(y_true, y_pred)\n        label_with_auc = f\"{label} AUC={auc:.3f}\"\n\n        # Plot ROC curve\n        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n        sns.lineplot(x=fpr, y=tpr, label=label_with_auc)\n        x = np.arange(0, 1.1, 0.1)  # Plot line for AUC=0.5\n        sns.lineplot(x=x, y=x, label=\"AUC=0.5\")\n        plt.title(f\"ROC on {dataset} dataset\")\n        plt.xlabel(\"False Positive Rate\")\n        plt.ylabel(\"True Positive Rate\")\n        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)  # Set legend outside plot\n        plt.show()\n        print(f\"On {dataset} dataset, {label} model - logloss = {logloss:.3f}, AUC = {auc:.3f}\")\n        \n        # Create result dataframe\n        result_dict = {\n            \"Model\": label,\n            'Dataset': dataset,\n            'log_loss': logloss,\n            'AUC': auc\n        }\n        \n        global i\n        df_result = pd.DataFrame(result_dict, index=[i])\n        i += 1\n        return df_result\n        \n    else:\n        # Plot ROC curves for multiple models\n        plt.figure(figsize=(12, 8))\n        for k, y in enumerate(y_pred):\n            fpr, tpr, thresholds = roc_curve(y_true, y)\n            auc = roc_auc_score(y_true, y)\n            label_with_auc = f\"{label[k]} AUC={auc:.3f}\"\n            sns.lineplot(x=fpr, y=tpr, label=label_with_auc)\n\n        x = np.arange(0, 1.1, 0.1)\n        sns.lineplot(x=x, y=x, label=\"AUC=0.5\")\n        plt.title(\"Combined ROC Curves\")\n        plt.xlabel('False Positive Rate')\n        plt.ylabel(\"True Positive Rate\")\n        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:04:35.972670Z","iopub.execute_input":"2025-12-10T09:04:35.973027Z","iopub.status.idle":"2025-12-10T09:04:35.985235Z","shell.execute_reply.started":"2025-12-10T09:04:35.973002Z","shell.execute_reply":"2025-12-10T09:04:35.984150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_optimal_hyperparameters(model, param_distributions, X_train, y_train, \n                                 cv=5, n_iter=100, n_jobs=-1, scoring='roc_auc',\n                                 random_state=42, verbose=1):\n    \"\"\"\n    Find optimal hyperparameters using Randomized Search with Cross-Validation\n    \n    Parameters:\n    -----------\n    model : sklearn estimator\n        Machine learning model\n    param_distributions : dict\n        Parameter distributions for random search\n    X_train : pandas DataFrame or numpy array\n        Training features\n    y_train : pandas Series or numpy array\n        Training labels\n    cv : int, default=5\n        Number of cross-validation folds\n    n_iter : int, default=100\n        Number of parameter settings sampled\n    n_jobs : int, default=-1\n        Number of jobs to run in parallel\n    scoring : str, default='roc_auc'\n        Scoring metric\n    random_state : int, default=42\n        Random seed for reproducibility\n    verbose : int, default=1\n        Verbosity level\n    \n    Returns:\n    --------\n    random_search : RandomizedSearchCV object\n        Fitted random search object\n    \"\"\"\n    \n    from sklearn.model_selection import RandomizedSearchCV\n    import numpy as np\n    \n    print(\"=\" * 60)\n    print(\"Starting Randomized Search for Optimal Hyperparameters\")\n    print(\"=\" * 60)\n    \n    # Data validation\n    print(\"\\n1. Data Validation:\")\n    print(f\"   Training samples: {X_train.shape[0]}\")\n    print(f\"   Features: {X_train.shape[1]}\")\n    print(f\"   Target distribution:\")\n    print(f\"     - Class 0: {(y_train == 0).sum()} samples\")\n    print(f\"     - Class 1: {(y_train == 1).sum()} samples\")\n    \n    # Check for NaN and infinite values\n    nan_count = np.isnan(X_train.values).sum() if hasattr(X_train, 'values') else 0\n    inf_count = np.isinf(X_train.values).sum() if hasattr(X_train, 'values') else 0\n    \n    if nan_count > 0:\n        print(f\"\\n⚠️ Warning: Found {nan_count} NaN values in features\")\n        X_train = X_train.fillna(X_train.median())  # Simple imputation\n        print(\"   Applied median imputation for NaN values\")\n    \n    if inf_count > 0:\n        print(f\"\\n⚠️ Warning: Found {inf_count} infinite values in features\")\n        # Replace infinite values with large finite values\n        X_train = X_train.replace([np.inf, -np.inf], np.nan)\n        X_train = X_train.fillna(X_train.median())\n        print(\"   Replaced infinite values with median\")\n    \n    # Initialize Randomized Search\n    print(f\"\\n2. Randomized Search Configuration:\")\n    print(f\"   Model: {model.__class__.__name__}\")\n    print(f\"   CV folds: {cv}\")\n    print(f\"   Iterations: {n_iter}\")\n    print(f\"   Scoring metric: {scoring}\")\n    print(f\"   Parameter space: {len(param_distributions)} parameters\")\n    \n    random_search = RandomizedSearchCV(\n        estimator=model,\n        param_distributions=param_distributions,\n        n_iter=n_iter,\n        cv=cv,\n        scoring=scoring,\n        n_jobs=n_jobs,\n        random_state=random_state,\n        verbose=verbose,\n        refit=True,\n        return_train_score=True\n    )\n    \n    # Perform the search\n    print(f\"\\n3. Training in progress...\")\n    try:\n        random_search.fit(X_train, y_train)\n        \n        print(f\"\\n4. Results Summary:\")\n        print(f\"   Best score: {random_search.best_score_:.4f}\")\n        print(f\"   Best parameters:\")\n        for param, value in random_search.best_params_.items():\n            print(f\"     - {param}: {value}\")\n        \n        # Additional information\n        print(f\"\\n5. Additional Information:\")\n        print(f\"   Search completed in: {random_search.refit_time_:.2f} seconds\")\n        print(f\"   Best estimator: {random_search.best_estimator_}\")\n        \n        # Check for overfitting\n        if hasattr(random_search, 'cv_results_'):\n            train_scores = random_search.cv_results_['mean_train_score']\n            test_scores = random_search.cv_results_['mean_test_score']\n            if len(train_scores) > 0:\n                gap = np.mean(train_scores) - np.mean(test_scores)\n                print(f\"   Train-test gap: {gap:.4f}\")\n                if gap > 0.1:\n                    print(\"   ⚠️ Warning: Possible overfitting detected\")\n        \n    except Exception as e:\n        print(f\"\\n❌ Error during training: {str(e)}\")\n        print(f\"   Error type: {type(e).__name__}\")\n        \n        # Debug information\n        if hasattr(e, 'args'):\n            print(f\"   Error details: {e.args}\")\n        \n        raise\n    \n    print(\"=\" * 60)\n    print(\"Randomized Search Completed\")\n    print(\"=\" * 60)\n    \n    return random_search","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:04:45.426769Z","iopub.execute_input":"2025-12-10T09:04:45.427100Z","iopub.status.idle":"2025-12-10T09:04:45.441558Z","shell.execute_reply.started":"2025-12-10T09:04:45.427081Z","shell.execute_reply":"2025-12-10T09:04:45.440358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nimport time\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\n\nprint(\"=== LightGBM Simplified Training ===\")\n\n# 1. Data preprocessing\nprint(\"Preprocessing data...\")\nX_train_filled = X_train.fillna(X_train.median())\nX_val_filled = X_val.fillna(X_train.median())\n\n# 2. Create and train model\nprint(\"Starting training...\")\nstart_time = time.time()\n\n# Use basic parameters\nmodel_lgb = lgb.LGBMClassifier(\n    n_estimators=500,      # Reduce number of trees\n    learning_rate=0.05,    # Increase learning rate\n    max_depth=5,           # Reduce depth\n    random_state=42,\n    verbose=-1,            # No verbose output\n    n_jobs=-1              # Use all CPU cores\n)\n\n# Train directly without complex parameters\nmodel_lgb.fit(X_train_filled, y_train)\n\ntraining_time = time.time() - start_time\nprint(f\"Training completed. Time taken: {training_time:.2f} seconds\")\n\n# 3. Predict\nprint(\"Making predictions...\")\ny_pred = model_lgb.predict_proba(X_val_filled)[:, 1]\n\n# 4. Evaluate\nauc_score = roc_auc_score(y_val, y_pred)\nprint(f\"Validation set AUC: {auc_score:.4f}\")\n\n# 5. Save results\nif 'labels' in locals():\n    labels.append(\"LightGBM\")\nif 'y_preds' in locals():\n    y_preds.append(y_pred)\n\nprint(\"✅ LightGBM training completed successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T09:04:48.414278Z","iopub.execute_input":"2025-12-10T09:04:48.414564Z","iopub.status.idle":"2025-12-10T09:04:58.461571Z","shell.execute_reply.started":"2025-12-10T09:04:48.414544Z","shell.execute_reply":"2025-12-10T09:04:58.460626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature names sorted by importance\nbest_model = model_lgb\nfeat_imp = best_model.feature_importances_\nfeat_indices = np.argsort(feat_imp)[::-1]\nimportant_feat = X_train.columns[feat_indices]\nimportant_feat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save feature importance results\nimportant_feat_df = pd.DataFrame({'feat_name': important_feat, 'feat_imp': feat_imp[feat_indices]})\nimportant_feat_df.to_csv('./output/'+dataset+'_feat_imp.csv', index=False, encoding='utf-8')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Draw a ranking map of the top 50 features with the highest importance scores.\ntop = 50\ntop_indices = feat_indices[:top]\nmost_important_feat = X_train.columns[top_indices]\nplt.figure(figsize=(7, 12))\nsns.barplot(x=feat_imp[top_indices], y=most_important_feat)\nplt.title('Feature Importance')\nplt.xlabel('Importance')\nplt.ylabel(\"Feature names\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#submission\npd.DataFrame({'ID': test_id, 'TARGET': y_test_pred}).to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}